# The ants are striking again! 

Execute 

``` 
python main.py 
``` 

to contemplate their machinations. 

# References 

+ This [book](https://www.distributed-systems.net/index.php/books/ds3/), by Maarten van Steen, provides an amenable introduction to the field of distributed systems, covering, in Chapter 4, particularly, transient (MPI, for instance) and persistent (as the message-queuing systems) communication middlewares. 
+ The Chapter 3 of this [book](https://www.cs.usfca.edu/~peter/ipp2/index.html), on the other hand, by Peter Pacheco, introduces sympatically the idyiossincrasies of parallel programming in distributed systems; emphatically, it explicitly distinguishes the inconveniences tied to the distribution of processes across multiple machines. 

# Documents 

The [report](./report/main.pdf) is available; it describes our modelling decisions. Additionally, we recorded a video to contemplate the implementation of our pipeline in the cloud; check [this](./AWS_Services_TJF.mp4) to watch it. 

# Install 

Initially, you should also install PostgreSQL; for this, execute 

``` 
sudo apt-get install wget ca-certificates
wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" > /etc/apt/sources.list.d/pgdg.list'
sudo apt-get update
sudo apt-get install postgresql postgresql-contrib
``` 

and then 

``` 
sudo su - postgres
``` 

to enjoy root access to the data base. In this scenario, execute 

``` 
psql
CREATE USER {username} WITH PASSWORD '{password}'; # Your username! 
``` 

to instantiate an user. In Python, install the `psycopg2` package with 

``` 
pip install psycopg2-binary
``` 

to access the database; the commands 

```py 
import psycopg2 

conn = psycopg2.connect(database="postgres", 
	user="{username}", # Your username! 
	password="{password}" 
) 
``` 

will provide you this access. 

You should, moreover, use a JDBC Driver to execute PostgreSQL query in Spark; for this, execute 

``` 
wget https://jdbc.postgresql.org/download/postgresql-42.3.6.jar 
``` 

to capture it from the repository. 

In this application, we use NumPy, Celery, Spark and Dash and Tabulate; execute 

``` 
pip install numpy celery pyspark dash tabulate 
``` 

to install them. In this sense, use the command 

``` 
celery -A taesb worker --loglevel=INFO 
``` 

at the root directory of this repository to start posting the messages to the broker (at the moment we are writing this, there is a cluster in ECS AWS directed for this task; thus, this line is optional). Execute, in the next step, 

``` 
python main.py 
``` 

(check `python main.py --help` for the available parameters) to start a producer. If you want to start multiple scenarios, execute 

``` 
python run.py --instances N 
``` 

(`N` equals the quantity of simulations). In this sense, we should compute the summary statistics regarding the current state of the simulations; for this, we use Spark. Execute 

``` 
python process_spark.py 
```

to start its processes. Importantly, it computes these summary quantities each `min{25, process_time}` seconds, in which `process_time` equals the time required to process the data from all simulations. 
 
Execute, nextly, 

``` 
python app.py 
``` 

and check your browser in the provided site to contemplate the dashboard generated by Dash. 

# Instructions to execute Spark in an EMR cluster 

Initially, watch [this video](https://www.youtube.com/watch?v=r-ig8zpP3EM&pp=ugMICgJwdBABGAE%3D) to check how to setup a cluster. Then, create an S3 bucket named `bucket-name` (or other name; henceforth, we will use this name) and insert the files `postgres-42.3.6.jar` and the scripts [`SparkSubmit.py`](./taesb/utils/SparkSubmit.py) and [`SparkConf.py`](./taesb/SparkConf.py) within this bucket. Importantly, modify the file `SparkSubmit.py` to enable the exploitation of multiple instances; for this, instead of 

```
self.spark_session = SparkSession \
	.builder \
	.config("spark.jars", SPARK_JARS) \
	.config("spark.master", SPARK_MASTER) \
	.config("spark.ui.enabled", SPARK_UI_ENABLED) \
	.config("spark.driver.host", SPARK_DRIVER_HOST) \
	.getOrCreate() 
```

write 

``` 
self.spark_session = SparkSession \
        .builder \
        .config("spark.jars", SPARK_JARS) \
        .config("spark.driver.host", SPARK_DRIVER_HOST) \
        .getOrCreate() 
```

(the objective is that Spark's parallelism should trascend the master machine). 

Succeedingly, use the SSH key to access the cluster. 

Within this cluster, execute 

``` 
aws s3 cp s3://bucket-name/postgresql-42.3.6.jar . 
aws s3 cp s3://bucket-name/SparskSubmit.py . 
aws s3 cp s3://bucket-name/SparkConf.py 
``` 

to capture the files from the S3 bucket. Hence, execute 

``` 
spark-submit --driver-classpath postgresql-42.3.6.jar SparkSubmit.py 
``` 

(or `spark-submit --packages org.postgresql:postgresql:42-3-6.jar`) to start Spark's job. 

# Dockerfiles 

We designed Dockerfiles (check [this site](https://docs.docker.com/engine/install/ubuntu/) to install Docker in your machine) for the communication and the processing pipelines; they were evaluated in ECS AWS. To start Celery and send messages to the broker, initially build the image 

``` 
docker build . --file Dockerfile --tag taesb-celery 
``` 

and execute it, 

``` 
docker run --tty taesb-celery 
``` 

Spark's Dockerfile, on the other hand, is available at [`taesb/spark/Dockerfile`](./taesb/spark/Dockerfile); to execute the image, build it, 

``` 
cd taesb/spark/
docker build . --file Dockerfile --tag taesb-spark 
``` 

and execute 

``` 
docker run --tty taesbs-spark 
``` 
 
to start processing the data in the database. These steps combine and characterize our system, which alloes the distributed processing of data with appropriate and transparent communication mechanisms.    

# Wiki 

TAESB stands for The Ants' Empire Strikes Back; it is a pointer to George Lucas' movies. 
